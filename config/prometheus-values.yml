extraScrapeConfigs: |
  - job_name: 'spring-actuator'
    metrics_path: '/actuator/prometheus'
    scrape_interval: 5s
    static_configs:
    - targets:
      - devops-project.default.svc.cluster.local

alertmanagerFiles:
  alertmanager.yml:
    global:
      slack_api_url: https://hooks.slack.com/services/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

    receivers:
    # https://prometheus.io/docs/alerting/configuration/#slack_config
    - name: slack_alert
      slack_configs:
      - channel: "#kubernetes-cluster"
        #api_url: https://hooks.slack.com/services/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

    route:
      group_wait: 10s
      group_interval: 5m
      receiver: slack_alert
      repeat_interval: 3h


#route:
#  group_by: [cluster]
#  # If an alert isn't caught by a route, send it slack.
#  receiver: slack_alert
#  routes:
#  # Send severity=slack alerts to slack.
#  - match:
#      severity: slack
#    receiver: slack_alert

serverFiles:
  alerts:
    groups:
    - name: Instances
      rules:
        - alert: InstanceDown
          expr: up == 0
          for: 5m
          labels:
            severity: page
          annotations:
            description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
            summary: 'Instance {{ $labels.instance }} down'
        - alert: HighGCPause
          expr: jvm_gc_pause_seconds_count > 100
          labels:
            severity: page
          annotations:
            summary: 'JVM GC Pause Seconds Greater Than 100'
        - alert: DevOpsRequests
          expr: http_server_requests_seconds_count > 3
          labels:
            severity: page
          annotations:
            summary: 'Greater than 3 requests to service'
